# Mathematical Formulation of the RLAIF System

This document outlines the core mathematical principles governing our proposed bio-inspired learning architecture. The system integrates a high-level Vision-Language Model (VLM) as a teacher with a low-level cellular motor cortex.

## 1. The Cellular Brain's Policy (π)

The robot's "brain" is a Cell Assembly (`B`) defined by a set of learnable parameters (`θ`). It functions as a policy `π` that maps an environmental state (`S`) to a motor action (`A`). The state `S` is a vector containing sensory information like joint positions and torso orientation.

> $$ A = \pi(S; \theta) $$

This policy is realized through a distributed assembly of simple, binary-like cells, promoting robustness and emergent behavior.

## 2. The VLM Teacher's Evaluation Function (V)

The VLM acts as a sophisticated, non-linear evaluation function `V`. Instead of a traditional, human-engineered reward function, the teacher assesses the outcome of an action by observing the state of the environment. In our framework, it compares the state before (`I_before`) and after (`I_after`) the action to provide a scalar reward (`R`).

> $$ R = V(I_{before}, I_{after}) $$

This function `V` is prompted to provide a score based on an abstract, semantic goal (e.g., "stability," "progress"), effectively translating common-sense understanding into a tangible learning signal.

## 3. The Learning Rule: VLM-Guided Policy Gradient

To train the cellular brain, we update its parameters `θ` using a Policy Gradient algorithm. The update rule is designed to reinforce actions that lead to positive evaluations from the VLM teacher.

The objective is to find the parameters `θ` that maximize the expected reward:
> $$ J(\theta) = E_{\tau \sim \pi_{\theta}}[R(\tau)] $$
> where `τ` is a trajectory of states and actions generated by the policy.

The gradient of this objective with respect to the parameters `θ` guides the learning process:
> $$ \nabla_{\theta} J(\theta) \approx R \cdot \nabla_{\theta} \log(\pi(A|S; \theta)) $$

This leads to the final update rule, where `α` is the learning rate:

> $$ \theta_{new} \leftarrow \theta_{old} + \alpha \cdot R \cdot \nabla_{\theta} \log(\pi(A|S; \theta)) $$

**In essence:** The weights of the cells that contributed to a given action `A` are nudged in a positive direction if the VLM teacher provides a high reward (`R > 0`), and in a negative direction if the teacher provides a low or negative reward (`R < 0`). This process allows the system to learn complex behaviors from scratch, guided only by high-level, semantic feedback.
